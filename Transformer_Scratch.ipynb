{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Section 1: Encoder"],"metadata":{"id":"6SQTg6uXwRom"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qqt2oXCjkfZv"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from math import sqrt\n","import torch.nn.functional as F"]},{"cell_type":"code","source":["class Configuration():\n","  dim_token_emb= 768\n","  attention_probs_dropout_prob= 0.1\n","  classifier_dropout= None\n","  gradient_checkpointing= False\n","  hidden_act= \"gelu\"\n","  hidden_dropout_prob= 0.1\n","  hidden_size= 768\n","  initializer_range= 0.02\n","  intermediate_size= 3072\n","  layer_norm_eps= 1e-12\n","  max_position_embeddings= 512\n","  model_type= \"encoder\"\n","  num_attention_heads= 12\n","  num_hidden_layers= 12\n","  pad_token_id= 0\n","  position_embedding_type= \"absolute\"\n","  type_vocab_size= 2\n","  use_cache= True\n","  vocab_size= 30522\n"],"metadata":{"id":"5Qu4UfkPrOmI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["config = Configuration()"],"metadata":{"id":"Kn_sMALgqXvO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["config.dim_token_emb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jTc9hbcfr2C5","executionInfo":{"status":"ok","timestamp":1697707820098,"user_tz":-540,"elapsed":10,"user":{"displayName":"추제근","userId":"03658758388761689109"}},"outputId":"f4521c9a-5541-45ef-8e6f-d7b2b2d22678"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["768"]},"metadata":{},"execution_count":345}]},{"cell_type":"code","source":["def scaled_dot_product_attention(query, key, value):\n","    dim_k = query.size(-1)\n","    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n","    weights = F.softmax(scores, dim=-1)\n","    result = torch.bmm(weights, value)\n","    return result"],"metadata":{"id":"JNfz378Cmpcx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class AttentionHead(nn.Module):\n","    def __init__(self, embed_dim, head_dim):\n","        super().__init__()\n","        self.q = nn.Linear(embed_dim, head_dim)\n","        self.k = nn.Linear(embed_dim, head_dim)\n","        self.v = nn.Linear(embed_dim, head_dim)\n","\n","    def forward(self, hidden_state):\n","        attn_outputs = scaled_dot_product_attention(\n","            self.q(hidden_state), self.k(hidden_state), self.v(hidden_state))\n","        return attn_outputs\n","\n","    def forward(self, hidden_state, enc_out = None):\n","        if(enc_out == None):\n","          attn_outputs = scaled_dot_product_attention(\n","            self.q(hidden_state), self.k(hidden_state), self.v(hidden_state))\n","        else:\n","          attn_outputs = scaled_dot_product_attention(\n","              self.q(enc_out), self.k(enc_out), self.v(hidden_state))\n","        return attn_outputs"],"metadata":{"id":"EBf3qdWkmr8U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        embed_dim = config.hidden_size\n","        num_heads = config.num_attention_heads\n","        head_dim = embed_dim // num_heads\n","        if(config.model_type == \"encoder\"):\n","          self.heads = nn.ModuleList(\n","              [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]\n","          )\n","        else:\n","          self.heads = nn.ModuleList(\n","              [AttentionHead_mask(embed_dim, head_dim) for _ in range(num_heads)]\n","          )\n","        self.output_linear = nn.Linear(embed_dim, embed_dim)\n","\n","\n","    def forward(self, hidden_state, enc_out=None):\n","        if(enc_out == None):\n","          x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)\n","          x = self.output_linear(x)\n","        else:\n","          x = torch.cat([h(hidden_state, enc_out) for h in self.heads], dim=-1)\n","          x = self.output_linear(x)\n","        return x"],"metadata":{"id":"1WxgF137nMVf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FeedForward(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n","        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n","        self.gelu = nn.GELU()\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","\n","    def forward(self, x):\n","        x = self.linear_1(x)\n","        x = self.gelu(x)\n","        x = self.linear_2(x)\n","        x = self.dropout(x)\n","        return x"],"metadata":{"id":"ls8zgDsJnWvz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TransformerEncoderLayer(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n","        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n","        self.attention = MultiHeadAttention(config)\n","        self.feed_forward = FeedForward(config)\n","\n","    def forward(self, x):\n","        # Apply layer normalization and then copy input into query, key, value\n","        hidden_state = self.layer_norm_1(x)\n","        # Apply attention with a skip connection\n","        x = x + self.attention(hidden_state)\n","        # Apply feed-forward layer with a skip connection\n","        x = x + self.feed_forward(self.layer_norm_2(x))\n","        return x"],"metadata":{"id":"Yr9NBSq-nY4t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoder_layer = TransformerEncoderLayer(config)"],"metadata":{"id":"ew1MYfAtrGTM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Embeddings(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.token_embeddings = nn.Embedding(config.vocab_size,\n","                                             config.hidden_size)\n","        self.position_embeddings = nn.Embedding(config.max_position_embeddings,\n","                                                config.hidden_size)\n","        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n","        self.dropout = nn.Dropout()\n","\n","    def forward(self, input_ids):\n","        # Create position IDs for input sequence\n","        seq_length = input_ids.size(1)\n","        position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0)\n","        # Create token and position embeddings\n","        token_embeddings = self.token_embeddings(input_ids)\n","        position_embeddings = self.position_embeddings(position_ids)\n","        # Combine token and position embeddings\n","        embeddings = token_embeddings + position_embeddings\n","        embeddings = self.layer_norm(embeddings)\n","        embeddings = self.dropout(embeddings)\n","        return embeddings"],"metadata":{"id":"ne5o9AoWnbBq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TransformerEncoder(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.embeddings = Embeddings(config)\n","        self.layers = nn.ModuleList([TransformerEncoderLayer(config)\n","                                     for _ in range(config.num_hidden_layers)])\n","\n","    def forward(self, x):\n","        x = self.embeddings(x)\n","        for layer in self.layers:\n","            x = layer(x)\n","        return x"],"metadata":{"id":"BEDzUnqYneUn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoder = TransformerEncoder(config)"],"metadata":{"id":"ddIQg26cuyG2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"n68hGT2Wu19L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(device)\n","x = torch.randint(2, (2, config.max_position_embeddings))"],"metadata":{"id":"pg0qPIwFvRnR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["enc_out = encoder(x)"],"metadata":{"id":"2Z2HFsWmvS5k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["enc_out.size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DfU5GEvtvS2t","executionInfo":{"status":"ok","timestamp":1697707822865,"user_tz":-540,"elapsed":10,"user":{"displayName":"추제근","userId":"03658758388761689109"}},"outputId":"883d3780-cf81-4f34-acfc-efcf2a4e5419"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 512, 768])"]},"metadata":{},"execution_count":358}]},{"cell_type":"markdown","source":["## Section 2: Decoder"],"metadata":{"id":"lWIu-dM1wKBk"}},{"cell_type":"markdown","source":["### Do it your-self!"],"metadata":{"id":"NAoT5U6kwbMB"}},{"cell_type":"code","source":["config.model_type = 'decoder'"],"metadata":{"id":"mBqSNkqHDMcU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def scaled_dot_product_attention(query, key, value, mask=None):\n","    dim_k = query.size(-1)\n","    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n","\n","\n","    iter = scores.size()[0]\n","    x = scores.size()[1]\n","    y = scores.size()[2]\n","\n","    mask = torch.ones((iter, x, y))\n","\n","    for k in range(0, iter):\n","      for i in range(0, x):\n","        for j in range(i + 1, y):\n","          mask[k][i][j] = 0\n","\n","    if mask is not None:\n","        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n","    weights = F.softmax(scores, dim=-1)\n","    return weights.bmm(value)"],"metadata":{"id":"muxfDySengS6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class AttentionHead_mask(nn.Module):\n","    def __init__(self, embed_dim, head_dim):\n","        super().__init__()\n","        self.q = nn.Linear(embed_dim, head_dim)\n","        self.k = nn.Linear(embed_dim, head_dim)\n","        self.v = nn.Linear(embed_dim, head_dim)\n","\n","    def forward(self, hidden_state, enc_out=None):\n","        if(enc_out == None):\n","          mask = torch.ones((1, 2, 3))\n","          attn_outputs = scaled_dot_product_attention(\n","              self.q(hidden_state), self.k(hidden_state), self.v(hidden_state), mask)\n","        else:\n","          attn_outputs = scaled_dot_product_attention(\n","              self.q(enc_out), self.k(enc_out), self.v(hidden_state))\n","        return attn_outputs"],"metadata":{"id":"jX-lDlwIWxsk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TransformerDecoderLayer(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n","        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n","        self.layer_norm_3 = nn.LayerNorm(config.hidden_size)\n","        self.attention = MultiHeadAttention(config)\n","        self.feed_forward = FeedForward(config)\n","\n","    def forward(self, x, enc_out):\n","        # Apply layer normalization and then copy input into query, key, value\n","        hidden_state1 = self.layer_norm_1(x)\n","        # Apply attention with a skip connection\n","        x = x + self.attention(hidden_state1)\n","\n","        #decoder라서 추가\n","        #------------------------------------------------------------\n","\n","        hidden_state2 = self.layer_norm_2(x)\n","        x = x + self.attention(hidden_state2, enc_out)\n","\n","        #------------------------------------------------------------\n","        # Apply feed-forward layer with a skip connection\n","        x = x + self.feed_forward(self.layer_norm_3(x))\n","        return x"],"metadata":{"id":"CNSm0qoNA8qH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["decoder_layer = TransformerDecoderLayer(config)"],"metadata":{"id":"kYwIy9CAA8sA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TransformerDecoder(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.embeddings = Embeddings(config)\n","        self.layers = nn.ModuleList([TransformerDecoderLayer(config)\n","                                     for _ in range(config.num_hidden_layers)])\n","\n","    def forward(self, x, enc_out):\n","        x = self.embeddings(x)\n","        for layer in self.layers:\n","            x = layer(x, enc_out)\n","        return x"],"metadata":{"id":"yOIRzstSA8uC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(device)\n","y = torch.randint(2, (2, config.max_position_embeddings))\n","y.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jrWrsrn4Vt09","executionInfo":{"status":"ok","timestamp":1697707822866,"user_tz":-540,"elapsed":8,"user":{"displayName":"추제근","userId":"03658758388761689109"}},"outputId":"9bde5a80-a622-4de4-8858-40425d0bac57"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 512])"]},"metadata":{},"execution_count":365}]},{"cell_type":"code","source":["decoder = TransformerDecoder(config)"],"metadata":{"id":"m_eSQWLOIDzX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dec_out = decoder(y, enc_out)"],"metadata":{"id":"N1ZNlp57IYQq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dec_out.size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9MzyRXBj_ux8","executionInfo":{"status":"ok","timestamp":1697708416276,"user_tz":-540,"elapsed":27,"user":{"displayName":"추제근","userId":"03658758388761689109"}},"outputId":"4af91bf5-03f3-4428-b3e8-49d1aef75773"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 512, 768])"]},"metadata":{},"execution_count":368}]},{"cell_type":"code","source":["dec_out"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0fZ2AxqCIoEz","executionInfo":{"status":"ok","timestamp":1697708416277,"user_tz":-540,"elapsed":20,"user":{"displayName":"추제근","userId":"03658758388761689109"}},"outputId":"3954bcd9-133a-4e63-a36e-15951a8dc47c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[-4.4505, -3.2537,  1.6898,  ..., -1.9110,  4.5027,  0.1361],\n","         [ 2.2372, -2.7707,  3.4377,  ..., -3.4716,  3.1217, -1.9748],\n","         [ 1.8093, -3.9605,  4.9819,  ..., -3.6125, -0.2480, -1.2146],\n","         ...,\n","         [ 0.2960, -2.5317,  1.1145,  ..., -3.1193,  3.9762,  3.3910],\n","         [-2.5233, -2.2717,  1.6913,  ..., -2.0713,  3.6716, -1.1244],\n","         [ 1.4656,  2.3001,  1.3731,  ..., -2.5724,  2.1282,  2.3143]],\n","\n","        [[-0.7400, -3.8738, -1.1821,  ...,  1.6580, -2.2482, -5.4730],\n","         [-0.1189,  1.1953, -0.4870,  ..., -1.5399, -2.4965, -1.0879],\n","         [ 1.3713,  1.0016,  4.1497,  ..., -1.6893,  3.5379,  0.6695],\n","         ...,\n","         [ 1.0667,  1.2944,  2.4996,  ..., -3.2825,  2.9498, -0.4780],\n","         [-2.8186, -1.4097,  2.5078,  ...,  0.2320,  1.4502, -1.2938],\n","         [-1.9650, -1.4481,  2.0200,  ...,  1.7980, -0.5777, -0.3220]]],\n","       grad_fn=<AddBackward0>)"]},"metadata":{},"execution_count":369}]}]}